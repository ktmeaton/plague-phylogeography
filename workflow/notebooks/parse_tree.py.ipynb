{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Objectives\n",
    "\n",
    "1. Convert newick tree to PhyloXML format.\n",
    "1. Fixes internal node names from an IQTREE tree. \n",
    "   - internal nodes by default get named the confidence (ex. \"99/100\")\n",
    "   - this notebook instead gives them the name NODEi where i is an integer starting from 0.\n",
    "   - NODE0 therefore becomes the root.\n",
    "1. Convert tree to data frame.\n",
    "1. Combine metadata with tree data frame.\n",
    "1. Export:\n",
    "   - Tree (newick, nexus, phyloxml)\n",
    "   - Tree Dataframe (tsv)\n",
    "   - Color mapping (tsv)\n",
    "   - Lat Lon Mapping for Auspice (tsv)\n",
    "   - Lat Lon Mapping for BEAST (tsv)   \n",
    "   - Augur JSON\n",
    "   - Auspice JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats and plotting\n",
    "import pandas as pd\n",
    "from matplotlib import colors, gridspec\n",
    "import math\n",
    "\n",
    "# Phylogenetics\n",
    "from Bio import Phylo\n",
    "import treetime # Clock filter tips\n",
    "\n",
    "# System misc\n",
    "\n",
    "import time # Augur export\n",
    "import os\n",
    "import copy\n",
    "from augur import utils, export_v2\n",
    "import json\n",
    "\n",
    "# Logging to file\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Writing python objects to file\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "# Custom script variables\n",
    "SCRIPT_NAME = \"parse_tree\"\n",
    "\n",
    "try:\n",
    "    WILDCARDS = snakemake.wildcards\n",
    "    project_dir = os.getcwd()\n",
    "except NameError:\n",
    "    WILDCARDS = [\"all\", \"chromosome\", \"5\", \"prune\"]\n",
    "    project_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "    \n",
    "READS_ORIGIN = WILDCARDS[0]\n",
    "LOCUS_NAME = WILDCARDS[1]\n",
    "MISSING_DATA = WILDCARDS[2]\n",
    "PRUNE = WILDCARDS[3]\n",
    "\n",
    "\n",
    "NAME_COL = \"sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_path: /mnt/c/Users/ktmea/Projects/plague-phylogeography/results/iqtree/all/chromosome/filter5/iqtree_post.cf.reroot.nwk\n",
      "metadata_path: /mnt/c/Users/ktmea/Projects/plague-phylogeography/results/metadata/all/metadata.tsv\n",
      "aln path: /mnt/c/Users/ktmea/Projects/plague-phylogeography/results/snippy_multi/all/chromosome/filter5/snippy-multi.snps.aln\n",
      "auspice_config_path: /mnt/c/Users/ktmea/Projects/plague-phylogeography/results/config/auspice_config.json\n",
      "auspice_remote_dir_path: /mnt/c/Users/ktmea/Projects/plague-phylogeography/auspice/\n",
      "outdir: /mnt/c/Users/ktmea/Projects/plague-phylogeography/results/parse_tree/all/chromosome/filter5\n"
     ]
    }
   ],
   "source": [
    "config_dir = os.path.join(project_dir, \"results\", \"config\")\n",
    "results_dir = os.path.join(project_dir, \"results\")\n",
    "outdir = os.path.join(results_dir, \n",
    "                      SCRIPT_NAME,\n",
    "                      READS_ORIGIN,\n",
    "                      LOCUS_NAME,\n",
    "                      \"filter{}\".format(MISSING_DATA),\n",
    "                     )\n",
    "\n",
    "tree_path = os.path.join(\n",
    "    results_dir, \n",
    "    \"iqtree\", \n",
    "    READS_ORIGIN, \n",
    "    LOCUS_NAME,\n",
    "    PRUNE,\n",
    "    \"filter{}\".format(MISSING_DATA),    \n",
    "    \"iqtree.treefile\",\n",
    ")\n",
    "\n",
    "metadata_path = os.path.join(\n",
    "    results_dir,\n",
    "    \"metadata\",\n",
    "    READS_ORIGIN,  \n",
    "    \"metadata.tsv\"\n",
    ")\n",
    "\n",
    "aln_path = os.path.join(\n",
    "    results_dir,\n",
    "    \"snippy_multi\",\n",
    "    READS_ORIGIN,\n",
    "    LOCUS_NAME,\n",
    "    PRUNE,\n",
    "    \"filter{}\".format(MISSING_DATA),       \n",
    "    \"snippy-multi.snps.aln\",\n",
    ")\n",
    "\n",
    "auspice_config_path = os.path.join(config_dir, \"auspice_config.json\")\n",
    "auspice_remote_dir_path = os.path.join(project_dir, \"auspice/\")\n",
    "\n",
    "print(\"tree_path:\", tree_path)\n",
    "print(\"metadata_path:\", metadata_path)\n",
    "print(\"aln path:\", aln_path)\n",
    "print(\"auspice_config_path:\", auspice_config_path)\n",
    "print(\"auspice_remote_dir_path:\", auspice_remote_dir_path)\n",
    "print(\"outdir:\", outdir)\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Metadata Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the metadata\n",
    "metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "\n",
    "# Fix the problem with multiple forms of NA in the table\n",
    "# Consolidate missing data to the NO_DATA_CHAR\n",
    "metadata_df.fillna(NO_DATA_CHAR, inplace=True)\n",
    "\n",
    "# set the metadata index\n",
    "metadata_df.set_index(NAME_COL, inplace=True)\n",
    "\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Convert tree to PhyloXML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_xml = os.path.join(outdir, \"temp.xml\")\n",
    "try:\n",
    "    Phylo.convert(tree_path, \"newick\", tmp_xml, \"phyloxml\")\n",
    "except FileNotFoundError:\n",
    "    Phylo.convert(tree_alt_path, \"newick\", tmp_xml, \"phyloxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Fix internal node names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in XML tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the new tree\n",
    "tree = Phylo.read(tmp_xml, \"phyloxml\")\n",
    "\n",
    "# Deepest nodes first\n",
    "tree.ladderize(reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix node names and confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for integer node names\n",
    "node_i = 0\n",
    "\n",
    "# Iterate through the nodes in the tree\n",
    "for c in tree.find_clades():\n",
    "    # Check if the name has 'accidentally' become the confidence vals\n",
    "    ufboot_val = NO_DATA_CHAR    \n",
    "    scf_val = NO_DATA_CHAR\n",
    "    # If it's a named node, parse it\n",
    "    if c.name:\n",
    "        name_split = c.name.split(\"/\")\n",
    "        \n",
    "        # Check if the name has 'accidentally' become the confidence vals\n",
    "        if len(name_split) > 1:\n",
    "            # Name the internal node\n",
    "            c.name = \"NODE\" + str(node_i)    \n",
    "\n",
    "            # Assign confidence values     \n",
    "            ufboot_val = float(name_split[0])\n",
    "            scf_val = float(name_split[1])\n",
    "    # If it's not a named node, give it a name\n",
    "    else:\n",
    "        c.name = \"NODE\" + str(node_i) \n",
    "        # The one and only confidence is \"unknown\" and if UFboot\n",
    "        if len(c.confidences) > 0:\n",
    "            ufboot_val = c.confidences[0].value\n",
    "        c.confidences = []\n",
    "        \n",
    "    \n",
    "    ufboot_conf = Phylo.PhyloXML.Confidence(ufboot_val, type=\"ufboot\")\n",
    "    scf_conf = Phylo.PhyloXML.Confidence(scf_val, type=\"scf\")\n",
    "    \n",
    "    c.confidences.append(ufboot_conf)\n",
    "    c.confidences.append(scf_conf)\n",
    "        \n",
    "    # Increment node counter if not a terminal\n",
    "    if not c.is_terminal():\n",
    "        node_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, dpi=400)\n",
    "\n",
    "Phylo.draw(tree,\n",
    "           axes=ax1, \n",
    "           show_confidence=False, \n",
    "           label_func = lambda x:'', \n",
    "           do_show=False,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Convert tree to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the tree dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize empty dataframe\n",
    "tree_df = pd.DataFrame(columns = [\n",
    "    \"name\",\n",
    "    \"ufboot\", \n",
    "    \"scf\", \n",
    "    \"branch_length\", \n",
    "    \"node_type\",\n",
    "    \"coord_x\", \n",
    "    \"coord_y\",\n",
    "]\n",
    ")\n",
    "\n",
    "for c in tree.find_clades():\n",
    "    if not c.branch_length: c.branch_length = 0.0\n",
    "    \n",
    "    node_data = {\n",
    "        \"name\" : c.name,\n",
    "        \"ufboot\" : [conf.value for conf in c.confidences if conf.type==\"ufboot\"][0],\n",
    "        \"scf\" :  [conf.value for conf in c.confidences if conf.type==\"scf\"][0],\n",
    "        \"node_type\" :  NO_DATA_CHAR,\n",
    "        \"branch_length\" :  c.branch_length,\n",
    "        \"coord_x\" :  NO_DATA_CHAR,\n",
    "        \"coord_y\" :  NO_DATA_CHAR,\n",
    "    }\n",
    "    \n",
    "    tree_df = tree_df.append(node_data, ignore_index=True)\n",
    "\n",
    "# Set the index to the node name\n",
    "tree_df.set_index(\"name\", inplace=True)\n",
    "\n",
    "# Visualize data frame\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add plotting x and y coordinates of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_posns = get_x_positions(tree)\n",
    "y_posns = get_y_positions(tree)\n",
    "\n",
    "# Add x and y coordinates as other attributes\n",
    "for c in tree.find_clades():\n",
    "    # x coordinates will be of branch length units\n",
    "    coord_x = [value for key,value in x_posns.items() if key.name == c.name][0]\n",
    "    # y coordinates will be arbitrary, based on number of tips\n",
    "    coord_y = [value for key,value in y_posns.items() if key.name == c.name][0]\n",
    "    # Add data to tree dataframe\n",
    "    tree_df.at[c.name, 'coord_x'] = coord_x\n",
    "    tree_df.at[c.name, 'coord_y'] = coord_y    \n",
    "\n",
    "# Visualize dataframe\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tree.find_clades():   \n",
    "    # Default Color\n",
    "    node_type = \"internal\"\n",
    "    # Terminal branches will be grey\n",
    "    if c.is_terminal():\n",
    "        node_type = \"terminal\"\n",
    "    tree_df.at[c.name, \"node_type\"] = node_type          \n",
    "\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Combine metadata into tree dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse metadata into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse the metadata\n",
    "metadata_df = pd.read_csv(metadata_path, sep='\\t')\n",
    "\n",
    "# Fix the problem with multiple forms of NA in the table\n",
    "# Consolidate missing data to the NO_DATA_CHAR\n",
    "metadata_df.fillna(NO_DATA_CHAR, inplace=True)\n",
    "\n",
    "# set the metadata index\n",
    "metadata_df.set_index(NAME_COL, inplace=True)\n",
    "\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add metadata to tree and tree dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the different metadata attributes\n",
    "for attr in metadata_df.columns:\n",
    "    # Initialize an empty column for the attribute\n",
    "    tree_df[attr.lower()] = [NO_DATA_CHAR for row in range(0,len(tree_df))]\n",
    "    # Iterate over the nodes in the tree\n",
    "    for c in tree.find_clades():\n",
    "        # Initialize to empty\n",
    "        attr_val = NO_DATA_CHAR\n",
    "        # If the node shows up in the metadata\n",
    "        if c.name in metadata_df.index:\n",
    "            tree_df.at[c.name, attr.lower()] = metadata_df[attr][c.name]\n",
    "            attr_val = metadata_df[attr][c.name]         \n",
    "\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Reference Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tree_df.columns:\n",
    "    if col in REF_META:\n",
    "        tree_df.at[\"Reference\", col] = REF_META[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make NA values consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the problem with multiple forms of NA in the table\n",
    "# Consolidate missing data to the NO_DATA_CHAR\n",
    "tree_df.fillna(NO_DATA_CHAR, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ?. Add Metadata As Node Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tree.find_clades():\n",
    "    c.comment = \"\"\n",
    "    for col in tree_df.columns:\n",
    "        col_val = tree_df[col][c.name]   \n",
    "        # Test for non ascii\n",
    "        if type(col_val) == str and not col_val.isascii():\n",
    "            col_val = col_val.encode(\"unicode_escape\")     \n",
    "        # Add comment\n",
    "        if not hasattr(c, \"comment\") or not c.comment:\n",
    "            c.comment = \"&{}={}\".format(col, col_val)\n",
    "        else:\n",
    "            c.comment += \",{}={}\".format(col, col_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tree and tree dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "out_path_df = os.path.join(outdir, SCRIPT_NAME + \".tsv\" )\n",
    "out_path_pickle_df = os.path.join(outdir,  SCRIPT_NAME + \".df.obj\" )\n",
    "\n",
    "tree_df.to_csv(out_path_df, sep=\"\\t\")\n",
    "with open(out_path_pickle_df,\"wb\") as outfile:\n",
    "    dill.dump(tree_df, outfile)\n",
    "\n",
    "# Nexus\n",
    "out_path_nexus = os.path.join(outdir, SCRIPT_NAME + \".nexus\" )\n",
    "Phylo.write(tree, out_path_nexus, 'nexus', format_branch_length='%1.{}f'.format(BRANCH_LEN_SIG_DIG))\n",
    "\n",
    "# Dill object\n",
    "out_path_dill_tree = os.path.join(outdir,  SCRIPT_NAME + \".phylo.obj\" )\n",
    "with open(out_path_dill_tree,\"wb\") as outfile:\n",
    "    dill.dump(tree, outfile)\n",
    "    \n",
    "# Newick (remove comments)\n",
    "for c in tree.find_clades(): c.comment = None\n",
    "out_path_nwk = os.path.join(outdir, SCRIPT_NAME + \".nwk\" )\n",
    "Phylo.write(tree, out_path_nwk, 'newick', format_branch_length='%1.{}f'.format(BRANCH_LEN_SIG_DIG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save lat and lon\n",
    "\n",
    "Write an output tsv for auspice that is:  \n",
    "resolution   name   lat    lon\n",
    "\n",
    "Write an output tsv for beast that is: \n",
    "sample lat\n",
    "\n",
    "Write an output tsv for beast that is: \n",
    "sample lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_dict_auspice = {\"country\": {}, \"province\": {}}\n",
    "latlon_dict_beast = {}\n",
    "\n",
    "out_path_lat_lon_auspice = os.path.join(outdir, SCRIPT_NAME + \"_lat-lon_auspice.tsv\")\n",
    "out_path_lat_beast = os.path.join(outdir, SCRIPT_NAME + \"_lat_beast.tsv\")\n",
    "out_path_lon_beast = os.path.join(outdir, SCRIPT_NAME + \"_lon_beast.tsv\")\n",
    "\n",
    "for c in tree.find_clades():\n",
    "    # For auspice\n",
    "    # Iterate through each geographic level\n",
    "    for level in latlon_dict_auspice:\n",
    "        # Store the location name\n",
    "        loc = tree_df[level][c.name]\n",
    "        # If the node has location data and is new\n",
    "        if loc != NO_DATA_CHAR and loc not in latlon_dict_auspice[level]:\n",
    "            latlon_dict_auspice[level][loc] = {}\n",
    "            latlon_dict_auspice[level][loc][\"lat\"] = tree_df[level + \"_lat\"][c.name]\n",
    "            latlon_dict_auspice[level][loc][\"lon\"] = tree_df[level + \"_lon\"][c.name]\n",
    "    \n",
    "    # For beast\n",
    "    # skip internal nodes\n",
    "    if not c.is_terminal(): continue\n",
    "    # Get latitude and longitude\n",
    "    lat = tree_df[\"province_lat\"][c.name]\n",
    "    if lat == NO_DATA_CHAR:\n",
    "        lat = tree_df[\"country_lat\"][c.name]\n",
    "    lon = tree_df[\"province_lon\"][c.name]\n",
    "    if lon == NO_DATA_CHAR:\n",
    "        lon = tree_df[\"country_lon\"][c.name] \n",
    "    \n",
    "    latlon_dict_beast[c.name] = {\"lat\": lat, \"lon\": lon}    \n",
    "\n",
    "with open(out_path_lat_lon_auspice, \"w\") as outfile:\n",
    "    for level in latlon_dict_auspice:\n",
    "        for loc in latlon_dict_auspice[level]:\n",
    "            outfile.write(level.lower() + \"\\t\"\n",
    "                         + loc + \"\\t\"\n",
    "                         + str(latlon_dict_auspice[level][loc][\"lat\"]) + \"\\t\"\n",
    "                         + str(latlon_dict_auspice[level][loc][\"lon\"]) + \"\\n\"\n",
    "                 )\n",
    "            \n",
    "with open(out_path_lat_beast, \"w\") as latfile:\n",
    "    with open(out_path_lon_beast, \"w\") as lonfile:          \n",
    "        for sample in latlon_dict_beast:\n",
    "            latfile.write(sample + \"\\t\" + str(latlon_dict_beast[sample][\"lat\"]) + \"\\n\")\n",
    "            lonfile.write(sample + \"\\t\" + str(latlon_dict_beast[sample][\"lon\"]) + \"\\n\")\n",
    "\n",
    "# For clarity with auspice        \n",
    "auspice_latlons_path = out_path_lat_lon_auspice        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dates\n",
    "\n",
    "Write an output tsv for beast that is:  \n",
    "sample    date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_dict_beast = {}\n",
    "out_path_dates_beast = os.path.join(outdir, SCRIPT_NAME + \"_dates_beast.tsv\")\n",
    "\n",
    "for c in tree.find_clades():\n",
    "    if not c.is_terminal(): continue\n",
    "    date_bp = str(tree_df[\"date_bp\"][c.name])\n",
    "    if date_bp == NO_DATA_CHAR: continue\n",
    "    date_bp = date_bp.lstrip(\"[\").rstrip(\"]\")\n",
    "    date_bp_split = [float(d) for d in date_bp.split(\":\")]\n",
    "    # Calculate the mean\n",
    "    date_bp_mean = sum(date_bp_split) / len(date_bp_split)\n",
    "    # Convert to positive\n",
    "    date_bp_pos = 0 - date_bp_mean\n",
    "    dates_dict_beast[c.name] = date_bp_pos\n",
    "\n",
    "with open(out_path_dates_beast, \"w\") as datefile:    \n",
    "    for sample in dates_dict_beast:\n",
    "        datefile.write(sample + \"\\t\" + str(dates_dict_beast[sample]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save colors\n",
    "\n",
    "Write an output tsv that is:\n",
    "attribute   attribute_val   hex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path_colors = os.path.join(outdir, SCRIPT_NAME + \"_colors.tsv\")\n",
    "file_colors = open(out_path_colors, \"w\")\n",
    "\n",
    "hex_dict = {}\n",
    "\n",
    "for attr in ATTRIBUTE_LIST:\n",
    "    # Create the color map\n",
    "    attr_key = attr.lower()\n",
    "    hex_dict[attr_key] = {}\n",
    "    for t in tree.get_terminals():\n",
    "        attr_val = tree_df[attr][t.name]\n",
    "        if attr_val not in hex_dict[attr_key] and attr_val != NO_DATA_CHAR:\n",
    "            hex_dict[attr_key][attr_val] = {}\n",
    "            \n",
    "    # Create the custom color map (pyplot)\n",
    "    cmap = plt.get_cmap(CONT_COLOR_PAL, len(hex_dict[attr_key]))\n",
    "    # Convert the color map to a list of RGB values\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    # Convert RGB values to hex colors\n",
    "    attr_hex = [colors.to_hex(col) for col in cmaplist]\n",
    "    \n",
    "    # Assign colors to value\n",
    "    for attr_val, attr_col in zip(hex_dict[attr_key], attr_hex):\n",
    "        hex_dict[attr_key][attr_val] = attr_col   \n",
    "\n",
    "for attr_key in hex_dict:\n",
    "    for attr_val in hex_dict[attr_key]:\n",
    "        file_colors.write(str(attr_key) + \"\\t\" + str(attr_val) + \"\\t\" + str(hex_dict[attr_key][attr_val]) + \"\\n\")\n",
    "\n",
    "file_colors.close()        \n",
    "\n",
    "# For clarity with auspice\n",
    "auspice_colors_path = out_path_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augur JSON\n",
    "\n",
    "  - alignment (empty)\n",
    "  - input_tree (tree_path)\n",
    "  - nodes (node_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augur_dict = augur_export(\n",
    "    tree_path=out_path_nwk, \n",
    "    aln_path=aln_path, \n",
    "    tree=tree, \n",
    "    tree_df=tree_df, \n",
    "    color_keyword_exclude=[\"color\", \"coord\", \"lat\", \"lon\"],\n",
    "    type_convert = {\n",
    "        \"branch_number\" : (lambda x : str(x))\n",
    "    },\n",
    ")\n",
    "\n",
    "out_path_augur_json = os.path.join(outdir, SCRIPT_NAME + \"_augur.json\" )\n",
    "utils.write_json(data=augur_dict, file_name=out_path_augur_json, indent=JSON_INDENT)\n",
    "\n",
    "print(augur_dict[\"nodes\"][\"Reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auspice JSON\n",
    "\n",
    "Manual edits of https://github.com/nextstrain/augur/blob/master/augur/export_v2.py\n",
    "\n",
    "This can then be used for auspice via:\n",
    "\n",
    "```\n",
    "HOST=\"localhost\" auspice view --datasetDir .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auspice_dict = auspice_export(\n",
    "    tree=tree, \n",
    "    augur_json_paths=[out_path_augur_json], \n",
    "    auspice_config_path=auspice_config_path, \n",
    "    auspice_colors_path=auspice_colors_path,\n",
    "    auspice_latlons_path=auspice_latlons_path, \n",
    "    )\n",
    "\n",
    "# Write outputs - For Local Rendering\n",
    "out_path_auspice_local_json = os.path.join(outdir, SCRIPT_NAME + \"_auspice.json\" )\n",
    "utils.write_json(data=auspice_dict, file_name=out_path_auspice_local_json, indent=JSON_INDENT, include_version=False)\n",
    "export_v2.validate_data_json(out_path_auspice_local_json)\n",
    "print(\"Validation successful for local JSON.\")\n",
    "\n",
    "# Write outputs - For Remote Rendering\n",
    "#out_path_auspice_remote_json = os.path.join(auspice_remote_dir_path, AUSPICE_PREFIX + SCRIPT_NAME.replace(\"_\",\"-\") + \".json\" )\n",
    "#utils.write_json(data=auspice_dict, file_name=out_path_auspice_remote_json, indent=JSON_INDENT, include_version=False)\n",
    "#export_v2.validate_data_json(out_path_auspice_remote_json)\n",
    "print(\"Validation successful for remote JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(tmp_xml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
